{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b0bced-d27a-4be8-a06a-c5a02d86d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935cac4-964a-4a0b-b13e-7cd444ea89f4",
   "metadata": {},
   "source": [
    "# Set hardware accelerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34958ce3-71c4-4c4a-941a-9a766ce95b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_GPU, TRAIN_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_K80, 1)\n",
    "\n",
    "DEPLOY_GPU, DEPLOY_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_K80, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b96d05-edae-4db9-bedc-52c97b81f50a",
   "metadata": {},
   "source": [
    "# Set pre-built containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09df84da-e595-45d7-8ff8-1f0eefbe6481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: us-docker.pkg.dev/vertex-ai/training/tf2-cpu.2-7:latest AcceleratorType.NVIDIA_TESLA_K80 1\n",
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-7:latest AcceleratorType.NVIDIA_TESLA_K80 1\n"
     ]
    }
   ],
   "source": [
    "TRAIN_VERSION = \"tf2-cpu.2-7\"\n",
    "DEPLOY_VERSION = \"tf2-cpu.2-7\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c470db-58d0-4fb8-897d-77b15cc309a6",
   "metadata": {},
   "source": [
    "# Set machine types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bf19846-04c1-4167-97d8-485695605a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30b0b3-949e-455e-8e57-517e0a4f5d26",
   "metadata": {},
   "source": [
    "# Initialize SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70e7b053-1f6a-4137-82c7-e1189b53fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    # your Google Cloud Project ID or number\n",
    "    # environment default used is not set\n",
    "    project='tii-sac-platform-sandbox-alpha',\n",
    "\n",
    "    # the Vertex AI region you will use\n",
    "    # defaults to us-central1\n",
    "    location='europe-west4',\n",
    "\n",
    "    # Google Cloud Storage bucket in same region as location\n",
    "    # used to stage artifacts\n",
    "    staging_bucket='gs://vertexai_staging_bucket',\n",
    "\n",
    "    # custom google.auth.credentials.Credentials\n",
    "    # environment default creds used if not set\n",
    "    # credentials=my_credentials,\n",
    "\n",
    "    # customer managed encryption key resource name\n",
    "    # will be applied to all Vertex AI resources if set\n",
    "    # encryption_spec_key_name=my_encryption_key_name,\n",
    "\n",
    "    # the name of the experiment to use to track\n",
    "    # logged metrics and parameters\n",
    "    experiment='titanic-classifier',\n",
    "\n",
    "    # description of the experiment above\n",
    "    experiment_description='VertexAI Demo for IIT'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9521500d-9d73-48b1-bc07-156089bb1d48",
   "metadata": {},
   "source": [
    "# Create/Register a Vertex AI managed Tabular Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00f003b9-6f49-4ab5-99fa-1d4d7c399c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create new\n",
    "#my_dataset = aiplatform.TabularDataset.create(\n",
    "#    display_name=\"Kaggle-titanic\", gcs_source=['gs://vertex_ai_demos1/datasets/titanic/train.csv', 'gs://vertex_ai_demos1/datasets/titanic/test.csv'])\n",
    "\n",
    "# To reference an existing one\n",
    "dataset = aiplatform.TabularDataset(\n",
    "    dataset_name='1405923528204615680',\n",
    "    project='tii-sac-platform-sandbox-alpha',\n",
    "    location='europe-west4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bea7a4-b534-4e11-a513-826fc7be6a43",
   "metadata": {},
   "source": [
    "# Define command args for training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18349842-60f2-445c-a1d6-092a82a18ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "JOB_PREFIX=\"xgboost-titanic-classifier-pkg-ar\"\n",
    "JOB_NAME=f\"{JOB_PREFIX}-{datetime.now()}\"\n",
    "BUCKET_NAME=\"gs://vertexai_staging_bucket\"\n",
    "\n",
    "if not TRAIN_NGPU or TRAIN_NGPU < 2:\n",
    "    TRAIN_STRATEGY = \"single\"\n",
    "else:\n",
    "    TRAIN_STRATEGY = \"mirror\"\n",
    "\n",
    "CMDARGS = [\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c59e5f2-184a-4ab8-9234-61054be366c0",
   "metadata": {},
   "source": [
    "# Prepare your custom code to use a VertexAI managed dataset\n",
    "\n",
    "## You need to rewrite your main function so that it reads the input data paths from environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c04770d3-6ec6-4fb7-ad5c-808ec8847fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../trainer/main2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../trainer/main2.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from datetime import date\n",
    "\n",
    "from loguru import logger\n",
    "from six.moves import cPickle as pickle\n",
    "from tensorflow.io import gfile\n",
    "\n",
    "basepath = os.path.dirname(__file__)\n",
    "trainer_path = os.path.abspath(os.path.join(basepath, \"..\"))\n",
    "sys.path.append(trainer_path)\n",
    "\n",
    "from trainer.model import Model\n",
    "\n",
    "# This code does just the training of the model\n",
    "# There is a seperate Vertex AI batch prediction job that handles\n",
    "# Predicing on the test dataset\n",
    "# Read environmental variables\n",
    "\n",
    "training_data_uri = os.environ[\"AIP_TRAINING_DATA_URI\"]\n",
    "validation_data_uri = os.environ[\"AIP_VALIDATION_DATA_URI\"]\n",
    "test_data_uri = os.environ[\"AIP_TEST_DATA_URI\"]\n",
    "output_uri = os.environ[\"AIP_MODEL_DIR\"]\n",
    "\n",
    "def train_model(config):\n",
    "    # Our model here is actually a class object\n",
    "    # The Object holds all of the necessary functionalities to train a\n",
    "    # XGBoost model eg. loading the training data is handled there\n",
    "    try:\n",
    "        model = Model()\n",
    "        model.train(config)\n",
    "        logger.info(\"Training job completed succesfully!\")\n",
    "    except Exception as e:\n",
    "        # Write out an error file. This will be returned as the failureReason in the\n",
    "        # DescribeTrainingJob result.\n",
    "        trc = traceback.format_exc()\n",
    "        # Printing this causes the exception to be in the training job logs, as well.\n",
    "        logger.info(\"Exception during task: \" +\n",
    "                    str(e) + \"\\n\" + trc, file=sys.stderr)\n",
    "        # A non-zero exit code causes the training job to be marked as Failed.\n",
    "        sys.exit(255)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path for the local dummy data.\n",
    "    # Normally input data is read from a cloud storage bucket\n",
    "    # and output/results are also written to a bucket\n",
    "    dummy_data_output_path = os.path.abspath(\n",
    "        os.path.join(basepath, \"..\", \"outputs\")\n",
    "    )\n",
    "    dummy_data_input_path = os.path.abspath(\n",
    "        os.path.join(basepath, \"..\", \"inputs\")\n",
    "    )\n",
    "    # Parse the command line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--data_input_path\",\n",
    "        type=str,\n",
    "        default=training_data_uri,\n",
    "        help=\"Data input (bucket) location.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--trainer_output_path\",\n",
    "        type=str,\n",
    "        default=output_uri,\n",
    "        help=\"Trainer code Output/result (bucket) location.\",\n",
    "    )\n",
    "    options, args = parser.parse_known_args()\n",
    "    # Set logging level to ERROR by default, change to DEBUG for more robust logging\n",
    "    logging.basicConfig(format=\"%(levelname)s:%(message)s\",\n",
    "                        level=logging.ERROR)\n",
    "    train_model(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22ddfccc-3719-447b-ac97-8b2699061094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9bf31f85-574a-4ea3-b4eb-13f519e1bac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/titanic_classifier/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b57624-8981-4e79-a4cc-949e7e6db1f0",
   "metadata": {},
   "source": [
    "# Run custom model training job in VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5d031a4-4182-4f09-b425-34646af61fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = \"europe-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest\"\n",
    "DEPLOY_IMAGE = \"europe-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-1:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a898ac19-7270-4b24-9c46-3d22968e1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.CustomPythonPackageTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    python_package_gcs_uri=\"gs://vertex_ai_demos1/vertexai_pipelines/titanic_classifier/trainer_pkg/trainer2-0.1.tar.gz\",\n",
    "    python_module_name=\"trainer2.main\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5261a196-6197-4008-8d98-127f57b3ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DISPLAY_NAME = \"titanic-classiciation-custom-training-\" + str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "33ac9a05-4749-4af9-9c0b-952ec118cf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://vertexai_staging_bucket/aiplatform-custom-training-2022-01-05-09:04:32.273 \n",
      "INFO:google.cloud.aiplatform.training_jobs:No dataset split provided. The service will use a default split.\n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west4/training/5534785403573239808?project=427665163432\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/427665163432/locations/europe-west4/trainingPipelines/5534785403573239808 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/427665163432/locations/europe-west4/trainingPipelines/5534785403573239808 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/427665163432/locations/europe-west4/trainingPipelines/5534785403573239808 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/427665163432/locations/europe-west4/trainingPipelines/5534785403573239808 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/427665163432/locations/europe-west4/trainingPipelines/5534785403573239808 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west4/training/6886568979226165248?project=427665163432\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/427665163432/locations/europe-west4/trainingPipelines/5534785403573239808 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob run completed. Resource name: projects/427665163432/locations/europe-west4/trainingPipelines/5534785403573239808\n",
      "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/427665163432/locations/europe-west4/models/5811983280051847168\n"
     ]
    }
   ],
   "source": [
    "model = job.run(\n",
    "        dataset=dataset,\n",
    "        model_display_name=MODEL_DISPLAY_NAME,\n",
    "        #bigquery_destination=\"bq://tii-sac-platform-sandbox-alpha:vertex_ai_demos\",\n",
    "        #gcs_destination=\"gs://vertex_ai_demos1/datasets/titanic\",\n",
    "        #args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        accelerator_count=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "72ef1071-7cfe-4b04-add7-93f92654d8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5811983280051847168'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc82f1-16b0-430f-a7a3-e48e8c76d0e1",
   "metadata": {},
   "source": [
    "# Get Batch Predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "91bf1c48-6c3e-4378-95bf-797a80bbe62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf.json_format import MessageToJson, ParseDict\n",
    "from google.protobuf.struct_pb2 import Struct, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ba6163d-e378-47fd-bce4-ef8d5e93fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "REGION = \"europe-west4\"\n",
    "PROJECT_ID = \"tii-sac-platform-sandbox-alpha\"\n",
    "# API Endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "# Vertex AI location root path for your dataset, model and endpoint resources\n",
    "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b8132b6-ec74-4bd4-8fcd-0fa3bb4041a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client options same for all services\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "\n",
    "def create_model_client():\n",
    "    client = aip.ModelServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_endpoint_client():\n",
    "    client = aip.EndpointServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_prediction_client():\n",
    "    client = aip.PredictionServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_job_client():\n",
    "    client = aip.JobServiceClient(client_options=client_options)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "07a1faac-4048-464b-a2e0-e8bce1193ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('model', <google.cloud.aiplatform_v1.services.model_service.client.ModelServiceClient object at 0x7f6b947ca950>)\n",
      "('endpoint', <google.cloud.aiplatform_v1.services.endpoint_service.client.EndpointServiceClient object at 0x7f6b947cac50>)\n",
      "('prediction', <google.cloud.aiplatform_v1.services.prediction_service.client.PredictionServiceClient object at 0x7f6b947ca9d0>)\n",
      "('job', <google.cloud.aiplatform_v1.services.job_service.client.JobServiceClient object at 0x7f6b947e6510>)\n"
     ]
    }
   ],
   "source": [
    "clients = {}\n",
    "clients[\"model\"] = create_model_client()\n",
    "clients[\"endpoint\"] = create_endpoint_client()\n",
    "clients[\"prediction\"] = create_prediction_client()\n",
    "clients[\"job\"] = create_job_client()\n",
    "\n",
    "for client in clients.items():\n",
    "    print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6004b882-af63-40a5-a3da-8826013b37fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"parent\": \"projects/tii-sac-platform-sandbox-alpha/locations/europe-west4\",\n",
      "  \"model\": {\n",
      "    \"displayName\": \"custom_job_XGB20220105091427\",\n",
      "    \"containerSpec\": {\n",
      "      \"imageUri\": \"gcr.io/cloud-aiplatform/prediction/xgboost-cpu.1-1:latest\",\n",
      "      \"ports\": [\n",
      "        {\n",
      "          \"containerPort\": 8080\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"artifactUri\": \"gs://vertexai_staging_bucket/aiplatform-custom-training-2022-01-05-09:04:32.273/model\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "DEPLOY_IMAGE = \"gcr.io/cloud-aiplatform/prediction/xgboost-cpu.1-1:latest\"\n",
    "model_artifact_dir = \"gs://vertexai_staging_bucket/aiplatform-custom-training-2022-01-05-09:04:32.273/model\"\n",
    "\n",
    "model = {\n",
    "    \"display_name\": \"custom_job_XGB\" + TIMESTAMP,\n",
    "    \"artifact_uri\": model_artifact_dir,\n",
    "    \"container_spec\": {\"image_uri\": DEPLOY_IMAGE, \"ports\": [{\"container_port\": 8080}]},\n",
    "}\n",
    "\n",
    "print(MessageToJson(aip.UploadModelRequest(parent=PARENT, model=model).__dict__[\"_pb\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d37c9b69-7280-4e40-9089-7cca47024972",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = clients[\"model\"].upload_model(parent=PARENT, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "61f4885f-1c30-4969-b267-53f696971a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"projects/427665163432/locations/europe-west4/models/3506140270838153216\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = request.result()\n",
    "\n",
    "print(MessageToJson(result.__dict__[\"_pb\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "42bc910f-828c-4434-b702-5550671be258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/427665163432/locations/europe-west4/models/3506140270838153216\n"
     ]
    }
   ],
   "source": [
    "# The full unique ID for the model version\n",
    "model_id = result.model\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c01450-ac6c-415a-bf9c-faffbfd4cfda",
   "metadata": {},
   "source": [
    "# Do a batch prediction with the deployed model\n",
    "\n",
    "The dataset used for batch predictions needs to preprocessed.\n",
    "\n",
    "So it's good to test that your dataset works before submitting a batch prediction job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "692f3e6a-5afd-4374-9d8e-00da40fd7712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Fsize</th>\n",
       "      <th>Single</th>\n",
       "      <th>SmallF</th>\n",
       "      <th>...</th>\n",
       "      <th>T_STONO</th>\n",
       "      <th>T_STONO2</th>\n",
       "      <th>T_STONOQ</th>\n",
       "      <th>T_SWPP</th>\n",
       "      <th>T_WC</th>\n",
       "      <th>T_WEP</th>\n",
       "      <th>T_X</th>\n",
       "      <th>Pc_1</th>\n",
       "      <th>Pc_2</th>\n",
       "      <th>Pc_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>881</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>882</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>883</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>884</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>885</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Survived  Sex   Age  SibSp  Parch      Fare  Fsize  Single  \\\n",
       "0         881         2    0  39.0      0      0  0.000000      1       1   \n",
       "1         882         2    0  49.0      0      0  0.000000      1       1   \n",
       "2         883         2    0  46.0      0      0  3.258097      1       1   \n",
       "3         884         2    0  49.0      0      0  3.258097      1       1   \n",
       "4         885         2    0  25.0      0      0  3.258097      1       1   \n",
       "\n",
       "   SmallF  ...  T_STONO  T_STONO2  T_STONOQ  T_SWPP  T_WC  T_WEP  T_X  Pc_1  \\\n",
       "0       0  ...        0         0         0       0     0      0    1     1   \n",
       "1       0  ...        0         0         0       0     0      0    1     1   \n",
       "2       0  ...        0         0         0       0     0      0    1     1   \n",
       "3       0  ...        0         0         0       0     0      0    1     1   \n",
       "4       0  ...        0         0         0       0     0      0    1     1   \n",
       "\n",
       "   Pc_2  Pc_3  \n",
       "0     0     0  \n",
       "1     0     0  \n",
       "2     0     0  \n",
       "3     0     0  \n",
       "4     0     0  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('../inputs/preprocessed_test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8cb7453f-affb-411b-826f-86822432509c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Fsize</th>\n",
       "      <th>Single</th>\n",
       "      <th>SmallF</th>\n",
       "      <th>MedF</th>\n",
       "      <th>LargeF</th>\n",
       "      <th>...</th>\n",
       "      <th>T_STONO</th>\n",
       "      <th>T_STONO2</th>\n",
       "      <th>T_STONOQ</th>\n",
       "      <th>T_SWPP</th>\n",
       "      <th>T_WC</th>\n",
       "      <th>T_WEP</th>\n",
       "      <th>T_X</th>\n",
       "      <th>Pc_1</th>\n",
       "      <th>Pc_2</th>\n",
       "      <th>Pc_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex   Age  SibSp  Parch  Fare  Fsize  Single  SmallF  MedF  LargeF  ...  \\\n",
       "0    0  39.0      0      0   0.0      1       1       0     0       0  ...   \n",
       "\n",
       "   T_STONO  T_STONO2  T_STONOQ  T_SWPP  T_WC  T_WEP  T_X  Pc_1  Pc_2  Pc_3  \n",
       "0        0         0         0       0     0      0    1     1     0     0  \n",
       "\n",
       "[1 rows x 66 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "y_test = test.Survived.values\n",
    "test.drop('Survived', axis=1, inplace=True)\n",
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "17354ec8-1701-4eec-ae79-e1d69079aff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., 39.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_vector = test.head(1).values\n",
    "single_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a5883a69-aad4-4aa6-a6e7-e7fa4104eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "bst = xgb.Booster()  # init model\n",
    "bst.load_model('../notebooks/model.bst')  # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "95843f5f-9b25-48b6-9ce9-4c2750d7f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_single = np.array(single_vector)\n",
    "test_DMatrix = xgb.DMatrix(np.vstack(test_single))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d32dd9a6-7aa5-492c-8a5c-94ea448f4d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49629012], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.predict(test_DMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "57acafa9-ed46-4e8e-8d9b-c3491b3cf99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('../inputs/dropped_f_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9620e-dd25-460d-acc0-42f4f016a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write test data as json to a bucket\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gcs_input_uri = \"gs://vertexai_batch_prediction_results/titanic/test.jsonl\"\n",
    "with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
    "    for i in test.values:\n",
    "        f.write(str(i) + \"\\n\")\n",
    "\n",
    "#! gsutil cat $gcs_input_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f387f623-06a8-4ad2-a763-3a8cca6be758",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../inputs/test.jsonl', \"w\") as f:\n",
    "    for i in test.values:\n",
    "        f.write(str(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b3ec67f1-f7e6-40ef-ba49-4f393e297459",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 4 (char 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2155/414562872.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../inputs/test.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2155/414562872.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../inputs/test.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 4 (char 3)"
     ]
    }
   ],
   "source": [
    "with open('../inputs/test.jsonl', \"rb\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7d5531a1-f1b7-46c8-8366-4dcd201285b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://vertex_ai_demos1/datasets/titanic/test.jsonl'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcs_input_uri = \"gs://vertex_ai_demos1/datasets/titanic/test.jsonl\"\n",
    "gcs_input_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fcbb46ba-2180-429e-a136-9ab75c9f43b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"parent\": \"projects/tii-sac-platform-sandbox-alpha/locations/europe-west4\",\n",
      "  \"batchPredictionJob\": {\n",
      "    \"displayName\": \"custom_job_XGB20220105091427\",\n",
      "    \"model\": \"projects/427665163432/locations/europe-west4/models/5811983280051847168\",\n",
      "    \"inputConfig\": {\n",
      "      \"instancesFormat\": \"jsonl\",\n",
      "      \"gcsSource\": {\n",
      "        \"uris\": [\n",
      "          \"gs://vertexai_batch_prediction_results/titanic/test.jsonl\"\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    \"modelParameters\": {\n",
      "      \"max_predictions\": 10000.0,\n",
      "      \"confidence_threshold\": 0.5\n",
      "    },\n",
      "    \"outputConfig\": {\n",
      "      \"predictionsFormat\": \"jsonl\",\n",
      "      \"gcsDestination\": {\n",
      "        \"outputUriPrefix\": \"gs://vertexai_batch_prediction_results/titanic\"\n",
      "      }\n",
      "    },\n",
      "    \"dedicatedResources\": {\n",
      "      \"machineSpec\": {\n",
      "        \"machineType\": \"n1-standard-2\"\n",
      "      },\n",
      "      \"startingReplicaCount\": 1,\n",
      "      \"maxReplicaCount\": 1\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model_id = \"projects/427665163432/locations/europe-west4/models/5811983280051847168\"\n",
    "model_parameters = Value(\n",
    "    struct_value=Struct(\n",
    "        fields={\n",
    "            \"confidence_threshold\": Value(number_value=0.5),\n",
    "            \"max_predictions\": Value(number_value=10000.0),\n",
    "        }\n",
    "    )\n",
    ")\n",
    "gcs_input_uri = \"\"\n",
    "batch_prediction_job = {\n",
    "    \"display_name\": \"custom_job_XGB\" + TIMESTAMP,\n",
    "    \"model\": model_id,\n",
    "    \"input_config\": {\n",
    "        \"instances_format\": \"jsonl\",\n",
    "        \"gcs_source\": {\"uris\": [\"gs://vertexai_batch_prediction_results/titanic/test.jsonl\"]},\n",
    "    },\n",
    "    \"model_parameters\": model_parameters,\n",
    "    \"output_config\": {\n",
    "        \"predictions_format\": \"jsonl\",\n",
    "        \"gcs_destination\": {\n",
    "            \"output_uri_prefix\": \"gs://vertexai_batch_prediction_results/titanic\"\n",
    "        },\n",
    "    },\n",
    "    \"dedicated_resources\": {\n",
    "        \"machine_spec\": {\"machine_type\": \"n1-standard-2\"},\n",
    "        \"starting_replica_count\": 1,\n",
    "        \"max_replica_count\": 1,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\n",
    "    MessageToJson(\n",
    "        aip.CreateBatchPredictionJobRequest(\n",
    "            parent=PARENT, batch_prediction_job=batch_prediction_job\n",
    "        ).__dict__[\"_pb\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b88adc94-c02a-43c8-a211-9675033ac46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = clients[\"job\"].create_batch_prediction_job(\n",
    "    parent=PARENT, batch_prediction_job=batch_prediction_job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9c78ca16-64eb-4c02-872b-cfcdba02f971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"projects/427665163432/locations/europe-west4/batchPredictionJobs/4440621800354742272\",\n",
      "  \"displayName\": \"custom_job_XGB20220105091427\",\n",
      "  \"model\": \"projects/427665163432/locations/europe-west4/models/5811983280051847168\",\n",
      "  \"inputConfig\": {\n",
      "    \"instancesFormat\": \"jsonl\",\n",
      "    \"gcsSource\": {\n",
      "      \"uris\": [\n",
      "        \"gs://vertexai_batch_prediction_results/titanic/test.jsonl\"\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"modelParameters\": {\n",
      "    \"max_predictions\": 10000.0,\n",
      "    \"confidence_threshold\": 0.5\n",
      "  },\n",
      "  \"outputConfig\": {\n",
      "    \"predictionsFormat\": \"jsonl\",\n",
      "    \"gcsDestination\": {\n",
      "      \"outputUriPrefix\": \"gs://vertexai_batch_prediction_results/titanic\"\n",
      "    }\n",
      "  },\n",
      "  \"dedicatedResources\": {\n",
      "    \"machineSpec\": {\n",
      "      \"machineType\": \"n1-standard-2\"\n",
      "    },\n",
      "    \"startingReplicaCount\": 1,\n",
      "    \"maxReplicaCount\": 1\n",
      "  },\n",
      "  \"manualBatchTuningParameters\": {},\n",
      "  \"state\": \"JOB_STATE_PENDING\",\n",
      "  \"createTime\": \"2022-01-05T12:33:17.872385Z\",\n",
      "  \"updateTime\": \"2022-01-05T12:33:17.872385Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(MessageToJson(request.__dict__[\"_pb\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8a58dd26-4d03-45e8-9f43-dd6161449fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/427665163432/locations/europe-west4/batchPredictionJobs/4440621800354742272\n"
     ]
    }
   ],
   "source": [
    "# The fully qualified ID for the batch job\n",
    "batch_job_id = request.name\n",
    "# The short numeric ID for the batch job\n",
    "batch_job_short_id = batch_job_id.split(\"/\")[-1]\n",
    "\n",
    "print(batch_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f94bd2b4-fd58-46c9-b2b2-e42a27941c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"projects/427665163432/locations/europe-west4/batchPredictionJobs/4440621800354742272\",\n",
      "  \"displayName\": \"custom_job_XGB20220105091427\",\n",
      "  \"model\": \"projects/427665163432/locations/europe-west4/models/5811983280051847168\",\n",
      "  \"inputConfig\": {\n",
      "    \"instancesFormat\": \"jsonl\",\n",
      "    \"gcsSource\": {\n",
      "      \"uris\": [\n",
      "        \"gs://vertexai_batch_prediction_results/titanic/test.jsonl\"\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"modelParameters\": {\n",
      "    \"max_predictions\": 10000.0,\n",
      "    \"confidence_threshold\": 0.5\n",
      "  },\n",
      "  \"outputConfig\": {\n",
      "    \"predictionsFormat\": \"jsonl\",\n",
      "    \"gcsDestination\": {\n",
      "      \"outputUriPrefix\": \"gs://vertexai_batch_prediction_results/titanic\"\n",
      "    }\n",
      "  },\n",
      "  \"dedicatedResources\": {\n",
      "    \"machineSpec\": {\n",
      "      \"machineType\": \"n1-standard-2\"\n",
      "    },\n",
      "    \"startingReplicaCount\": 1,\n",
      "    \"maxReplicaCount\": 1\n",
      "  },\n",
      "  \"manualBatchTuningParameters\": {},\n",
      "  \"state\": \"JOB_STATE_RUNNING\",\n",
      "  \"createTime\": \"2022-01-05T12:33:17.872385Z\",\n",
      "  \"startTime\": \"2022-01-05T12:33:17.899468Z\",\n",
      "  \"updateTime\": \"2022-01-05T12:33:17.899468Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "request = clients[\"job\"].get_batch_prediction_job(name=batch_job_id)\n",
    "print(MessageToJson(request.__dict__[\"_pb\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "923f34f1-2528-494e-b44b-742fe01038f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n",
      "The job has not completed: JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2155/3061792019.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' gsutil cat -h $folder/prediction*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "def get_latest_predictions(gcs_out_dir):\n",
    "    \"\"\" Get the latest prediction subfolder using the timestamp in the subfolder name\"\"\"\n",
    "    folders = !gsutil ls $gcs_out_dir\n",
    "    latest = \"\"\n",
    "    for folder in folders:\n",
    "        subfolder = folder.split(\"/\")[-2]\n",
    "        if subfolder.startswith(\"prediction-\"):\n",
    "            if subfolder > latest:\n",
    "                latest = folder[:-1]\n",
    "    return latest\n",
    "\n",
    "\n",
    "while True:\n",
    "    response = clients[\"job\"].get_batch_prediction_job(name=batch_job_id)\n",
    "    if response.state != aip.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(\"The job has not completed:\", response.state)\n",
    "        if response.state == aip.JobState.JOB_STATE_FAILED:\n",
    "            break\n",
    "    else:\n",
    "        folder = get_latest_predictions(\n",
    "            response.output_config.gcs_destination.output_uri_prefix\n",
    "        )\n",
    "        ! gsutil ls $folder/prediction*\n",
    "\n",
    "        ! gsutil cat -h $folder/prediction*\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad43ec-db2f-450c-b79c-f5f9d98b9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYED_NAME = \"titanic_classifier_deployed-\" + TIMESTAMP\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "if not DEPLOY_GPU:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        #accelerator_type=DEPLOY_GPU.name,\n",
    "        #accelerator_count=DEPLOY_NGPU,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )\n",
    "else:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        #accelerator_type=DEPLOY_COMPUTE.name,\n",
    "        #accelerator_count=0,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01045ea-b538-4e22-b540-0af33819c120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
