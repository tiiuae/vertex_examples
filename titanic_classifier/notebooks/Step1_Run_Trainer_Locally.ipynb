{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8cc3e9-0dae-4132-8911-419f8d34be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba01058-cf18-4405-b102-404308f4ab07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jupyter/titanic_classifier/notebooks', '/opt/conda/lib/python37.zip', '/opt/conda/lib/python3.7', '/opt/conda/lib/python3.7/lib-dynload', '', '/opt/conda/lib/python3.7/site-packages', '/opt/conda/lib/python3.7/site-packages/IPython/extensions', '/home/jupyter/.ipython', '/home/jupyter/titanic_classifier']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdebd38-3e84-42d3-accf-9c858f37b426",
   "metadata": {},
   "source": [
    "# Test your training package if it runs locally before going to cloud\n",
    "#### If you're running a large model or with large data f.ex. some sort of a Neural Network\n",
    "#### It's a good idea to do a dummy run with 2 epochs. If it runs for 2 epochs you can do the hard calculations in Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33faff6a-dd7e-46b0-bebc-4161b47b310f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 13:08:51.085047: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-05 13:08:51.085096: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from trainer import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b94e71b-6e8d-44fe-a7fc-e0f1d147e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create config\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6376f9-f512-4355-8942-b716b7ed17b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/titanic_classifier/notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2e2b09-eb9d-488b-8085-01a63dfdacad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "basepath = \"/home/jupyter/titanic_classifier\"\n",
    "dummy_data_output_path = os.path.abspath(\n",
    "    os.path.join(basepath, \"outputs\")\n",
    ")\n",
    "dummy_data_input_path = os.path.abspath(\n",
    "    os.path.join(basepath, \"inputs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f278761-2983-4610-95dc-202212be8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--data_input_path\",\n",
    "    type=str,\n",
    "    default=\"gs://vertex_ai_demos1/datasets/titanic\",#dummy_data_input_path,\n",
    "    help=\"Data input (bucket) location.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--trainer_output_path\",\n",
    "    type=str,\n",
    "    default=\"gs://vertex_ai_demos1/experiments/titanic_classifier\",#dummy_data_output_path,\n",
    "    help=\"Trainer code Output/result (bucket) location.\",\n",
    ")\n",
    "options, args = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15fd9bf4-020f-45fc-91ec-7827cc8453fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_input_path='gs://vertex_ai_demos1/datasets/titanic', trainer_output_path='gs://vertex_ai_demos1/experiments/titanic_classifier')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check args\n",
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6903c70c-3bbc-4fd6-a5a1-5d6cdaa46802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 13:08:53.016 | INFO     | trainer.model:train:38 - Load train dataset.\n",
      "2022-01-05 13:08:53.138 | INFO     | trainer.data_utils:reduce_mem_usage:54 - Mem. usage decreased to  0.04 Mb (47.8% reduction)\n",
      "2022-01-05 13:08:53.139 | INFO     | trainer.data_utils:load_dataset:86 - File null counts\n",
      "2022-01-05 13:08:53.144 | INFO     | trainer.data_utils:load_dataset:87 - PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "2022-01-05 13:08:53.267 | INFO     | trainer.data_utils:reduce_mem_usage:54 - Mem. usage decreased to  0.02 Mb (44.2% reduction)\n",
      "2022-01-05 13:08:53.268 | INFO     | trainer.data_utils:load_dataset:86 - File null counts\n",
      "2022-01-05 13:08:53.273 | INFO     | trainer.data_utils:load_dataset:87 - PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "2022-01-05 13:08:53.275 | INFO     | trainer.model:train:44 - Detect outliers from dataset.\n",
      "2022-01-05 13:08:53.283 | INFO     | trainer.model:train:47 - Drop outliers\n",
      "2022-01-05 13:08:53.286 | INFO     | trainer.model:train:53 - Concatenate train and test datasets.\n",
      "2022-01-05 13:08:53.293 | INFO     | trainer.model:train:55 - Handle missing values.\n",
      "2022-01-05 13:08:53.887 | INFO     | trainer.model:train:57 - Feature Engineering.\n",
      "2022-01-05 13:08:53.924 | INFO     | trainer.model:train:59 - Seperate train and test datasets again.\n",
      "2022-01-05 13:08:53.925 | INFO     | trainer.model:train:62 - Separate train features and target\n",
      "2022-01-05 13:08:53.929 | INFO     | trainer.model:train:66 - [0 1]\n",
      "2022-01-05 13:08:53.942 | INFO     | trainer.model:train:78 - Dataset columns(features) used for training: ['Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Fsize', 'Single', 'SmallF', 'MedF', 'LargeF', 'Title_0', 'Title_1', 'Title_2', 'Title_3', 'Em_C', 'Em_Q', 'Em_S', 'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_T', 'Cabin_X', 'T_A', 'T_A4', 'T_A5', 'T_AQ3', 'T_AQ4', 'T_AS', 'T_C', 'T_CA', 'T_CASOTON', 'T_FC', 'T_FCC', 'T_Fa', 'T_LINE', 'T_LP', 'T_PC', 'T_PP', 'T_PPP', 'T_SC', 'T_SCA3', 'T_SCA4', 'T_SCAH', 'T_SCOW', 'T_SCPARIS', 'T_SCParis', 'T_SOC', 'T_SOP', 'T_SOPP', 'T_SOTONO2', 'T_SOTONOQ', 'T_SP', 'T_STONO', 'T_STONO2', 'T_STONOQ', 'T_SWPP', 'T_WC', 'T_WEP', 'T_X', 'Pc_1', 'Pc_2', 'Pc_3']\n",
      "2022-01-05 13:08:53.953 | INFO     | trainer.model:train:81 - Start Cross-validation training for XGBoost Binary Classifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:08:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.87802+0.01519\ttest-auc:0.84580+0.02552\n",
      "[1]\ttrain-auc:0.89247+0.00894\ttest-auc:0.86217+0.03445\n",
      "[2]\ttrain-auc:0.89625+0.00817\ttest-auc:0.86705+0.03919\n",
      "[3]\ttrain-auc:0.90002+0.00825\ttest-auc:0.86976+0.03895\n",
      "[4]\ttrain-auc:0.90321+0.01192\ttest-auc:0.87176+0.03318\n",
      "[5]\ttrain-auc:0.90559+0.01277\ttest-auc:0.87353+0.03114\n",
      "[6]\ttrain-auc:0.90567+0.01297\ttest-auc:0.87443+0.03095\n",
      "[7]\ttrain-auc:0.90629+0.01295\ttest-auc:0.87515+0.03041\n",
      "[8]\ttrain-auc:0.90618+0.01240\ttest-auc:0.87532+0.03018\n",
      "[9]\ttrain-auc:0.90610+0.01283\ttest-auc:0.87654+0.03078\n",
      "[10]\ttrain-auc:0.90580+0.01289\ttest-auc:0.87658+0.03000\n",
      "[11]\ttrain-auc:0.90589+0.01309\ttest-auc:0.87706+0.03035\n",
      "[12]\ttrain-auc:0.90674+0.01283\ttest-auc:0.87838+0.03001\n",
      "[13]\ttrain-auc:0.90652+0.01295\ttest-auc:0.87771+0.03056\n",
      "[14]\ttrain-auc:0.90712+0.01288\ttest-auc:0.87727+0.03064\n",
      "[15]\ttrain-auc:0.90744+0.01279\ttest-auc:0.87698+0.03089\n",
      "[16]\ttrain-auc:0.90781+0.01223\ttest-auc:0.87720+0.03098\n",
      "[17]\ttrain-auc:0.90763+0.01193\ttest-auc:0.87723+0.03051\n",
      "[18]\ttrain-auc:0.90774+0.01182\ttest-auc:0.87748+0.03046\n",
      "[19]\ttrain-auc:0.90770+0.01199\ttest-auc:0.87778+0.03041\n",
      "[20]\ttrain-auc:0.90838+0.01321\ttest-auc:0.87782+0.03061\n",
      "[21]\ttrain-auc:0.90835+0.01318\ttest-auc:0.87789+0.03077\n",
      "[22]\ttrain-auc:0.90833+0.01297\ttest-auc:0.87780+0.03068\n",
      "[23]\ttrain-auc:0.90842+0.01282\ttest-auc:0.87767+0.03046\n",
      "[24]\ttrain-auc:0.90800+0.01294\ttest-auc:0.87726+0.02995\n",
      "[25]\ttrain-auc:0.90784+0.01321\ttest-auc:0.87742+0.03016\n",
      "[26]\ttrain-auc:0.90856+0.01273\ttest-auc:0.87702+0.03033\n",
      "[27]\ttrain-auc:0.90857+0.01265\ttest-auc:0.87696+0.03034\n",
      "[28]\ttrain-auc:0.90829+0.01279\ttest-auc:0.87707+0.03031\n",
      "[29]\ttrain-auc:0.90832+0.01298\ttest-auc:0.87699+0.03064\n",
      "[30]\ttrain-auc:0.90821+0.01296\ttest-auc:0.87723+0.03068\n",
      "[31]\ttrain-auc:0.90822+0.01297\ttest-auc:0.87697+0.03034\n",
      "[32]\ttrain-auc:0.90814+0.01285\ttest-auc:0.87667+0.03024\n",
      "[33]\ttrain-auc:0.90845+0.01265\ttest-auc:0.87683+0.03046\n",
      "[34]\ttrain-auc:0.90856+0.01282\ttest-auc:0.87676+0.03049\n",
      "[35]\ttrain-auc:0.90840+0.01287\ttest-auc:0.87684+0.03042\n",
      "[36]\ttrain-auc:0.90848+0.01285\ttest-auc:0.87696+0.03019\n",
      "[37]\ttrain-auc:0.90833+0.01278\ttest-auc:0.87695+0.03004\n",
      "[38]\ttrain-auc:0.90864+0.01258\ttest-auc:0.87717+0.03007\n",
      "[39]\ttrain-auc:0.90877+0.01264\ttest-auc:0.87698+0.03021\n",
      "[40]\ttrain-auc:0.90941+0.01239\ttest-auc:0.87627+0.03015\n",
      "[41]\ttrain-auc:0.90942+0.01244\ttest-auc:0.87635+0.03011\n",
      "[42]\ttrain-auc:0.90955+0.01241\ttest-auc:0.87604+0.03021\n",
      "[43]\ttrain-auc:0.90955+0.01242\ttest-auc:0.87627+0.03010\n",
      "[44]\ttrain-auc:0.90952+0.01243\ttest-auc:0.87634+0.02999\n",
      "[45]\ttrain-auc:0.90950+0.01243\ttest-auc:0.87596+0.03019\n",
      "[46]\ttrain-auc:0.90943+0.01250\ttest-auc:0.87609+0.03030\n",
      "[47]\ttrain-auc:0.90954+0.01263\ttest-auc:0.87583+0.03016\n",
      "[48]\ttrain-auc:0.90958+0.01264\ttest-auc:0.87560+0.03047\n",
      "[49]\ttrain-auc:0.90947+0.01282\ttest-auc:0.87559+0.03049\n",
      "[13:08:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.87802+0.01519\ttest-auc:0.84580+0.02552\n",
      "[1]\ttrain-auc:0.89248+0.00893\ttest-auc:0.86217+0.03445\n",
      "[2]\ttrain-auc:0.89625+0.00817\ttest-auc:0.86705+0.03919\n",
      "[3]\ttrain-auc:0.90004+0.00824\ttest-auc:0.86994+0.03888\n",
      "[4]\ttrain-auc:0.90331+0.01183\ttest-auc:0.87191+0.03313\n",
      "[5]\ttrain-auc:0.90566+0.01276\ttest-auc:0.87396+0.03131\n",
      "[6]\ttrain-auc:0.90625+0.01271\ttest-auc:0.87437+0.03032\n",
      "[7]\ttrain-auc:0.90705+0.01262\ttest-auc:0.87510+0.02971\n",
      "[8]\ttrain-auc:0.90695+0.01221\ttest-auc:0.87484+0.02948\n",
      "[9]\ttrain-auc:0.90704+0.01224\ttest-auc:0.87544+0.02934\n",
      "[10]\ttrain-auc:0.90695+0.01228\ttest-auc:0.87568+0.02870\n",
      "[11]\ttrain-auc:0.90712+0.01258\ttest-auc:0.87640+0.02930\n",
      "[12]\ttrain-auc:0.90864+0.01156\ttest-auc:0.87739+0.02884\n",
      "[13]\ttrain-auc:0.90860+0.01138\ttest-auc:0.87668+0.02933\n",
      "[14]\ttrain-auc:0.90904+0.01133\ttest-auc:0.87606+0.02907\n",
      "[15]\ttrain-auc:0.90944+0.01127\ttest-auc:0.87597+0.02933\n",
      "[16]\ttrain-auc:0.90982+0.01095\ttest-auc:0.87605+0.02970\n",
      "[17]\ttrain-auc:0.90959+0.01068\ttest-auc:0.87616+0.02933\n",
      "[18]\ttrain-auc:0.91167+0.00912\ttest-auc:0.87422+0.02737\n",
      "[19]\ttrain-auc:0.91163+0.00934\ttest-auc:0.87470+0.02737\n",
      "[20]\ttrain-auc:0.91223+0.01087\ttest-auc:0.87429+0.02735\n",
      "[21]\ttrain-auc:0.91227+0.01085\ttest-auc:0.87439+0.02734\n",
      "[22]\ttrain-auc:0.91229+0.01096\ttest-auc:0.87414+0.02731\n",
      "[23]\ttrain-auc:0.91280+0.01068\ttest-auc:0.87406+0.02719\n",
      "[24]\ttrain-auc:0.91145+0.01160\ttest-auc:0.87549+0.02805\n",
      "[25]\ttrain-auc:0.91147+0.01183\ttest-auc:0.87553+0.02810\n",
      "[26]\ttrain-auc:0.91197+0.01152\ttest-auc:0.87516+0.02854\n",
      "[27]\ttrain-auc:0.91282+0.01119\ttest-auc:0.87550+0.02917\n",
      "[28]\ttrain-auc:0.91245+0.01135\ttest-auc:0.87578+0.02908\n",
      "[29]\ttrain-auc:0.91270+0.01138\ttest-auc:0.87567+0.02920\n",
      "[30]\ttrain-auc:0.91273+0.01170\ttest-auc:0.87519+0.03001\n",
      "[31]\ttrain-auc:0.91273+0.01165\ttest-auc:0.87484+0.02989\n",
      "[32]\ttrain-auc:0.91276+0.01140\ttest-auc:0.87487+0.02993\n",
      "[33]\ttrain-auc:0.91304+0.01126\ttest-auc:0.87464+0.02994\n",
      "[34]\ttrain-auc:0.91311+0.01156\ttest-auc:0.87479+0.02986\n",
      "[35]\ttrain-auc:0.91294+0.01143\ttest-auc:0.87487+0.02975\n",
      "[36]\ttrain-auc:0.91303+0.01142\ttest-auc:0.87466+0.02957\n",
      "[37]\ttrain-auc:0.91319+0.01133\ttest-auc:0.87477+0.02959\n",
      "[38]\ttrain-auc:0.91317+0.01140\ttest-auc:0.87497+0.02960\n",
      "[39]\ttrain-auc:0.91316+0.01134\ttest-auc:0.87508+0.02968\n",
      "[40]\ttrain-auc:0.91321+0.01145\ttest-auc:0.87521+0.02988\n",
      "[41]\ttrain-auc:0.91311+0.01136\ttest-auc:0.87527+0.02989\n",
      "[42]\ttrain-auc:0.91323+0.01141\ttest-auc:0.87496+0.03028\n",
      "[43]\ttrain-auc:0.91336+0.01136\ttest-auc:0.87493+0.03060\n",
      "[44]\ttrain-auc:0.91347+0.01123\ttest-auc:0.87501+0.03038\n",
      "[45]\ttrain-auc:0.91342+0.01130\ttest-auc:0.87499+0.03046\n",
      "[46]\ttrain-auc:0.91349+0.01128\ttest-auc:0.87499+0.03067\n",
      "[47]\ttrain-auc:0.91375+0.01142\ttest-auc:0.87470+0.03044\n",
      "[48]\ttrain-auc:0.91371+0.01145\ttest-auc:0.87467+0.03059\n",
      "[49]\ttrain-auc:0.91364+0.01152\ttest-auc:0.87452+0.03045\n",
      "[13:08:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.87802+0.01519\ttest-auc:0.84580+0.02552\n",
      "[1]\ttrain-auc:0.89252+0.00888\ttest-auc:0.86226+0.03453\n",
      "[2]\ttrain-auc:0.89676+0.00832\ttest-auc:0.86707+0.03898\n",
      "[3]\ttrain-auc:0.90372+0.00953\ttest-auc:0.87212+0.03459\n",
      "[4]\ttrain-auc:0.90655+0.01283\ttest-auc:0.87422+0.03037\n",
      "[5]\ttrain-auc:0.90765+0.01281\ttest-auc:0.87519+0.02966\n",
      "[6]\ttrain-auc:0.90871+0.01243\ttest-auc:0.87475+0.02857\n",
      "[7]\ttrain-auc:0.90849+0.01347\ttest-auc:0.87603+0.02661\n",
      "[8]\ttrain-auc:0.90897+0.01279\ttest-auc:0.87522+0.02600\n",
      "[9]\ttrain-auc:0.90976+0.01184\ttest-auc:0.87595+0.02678\n",
      "[10]\ttrain-auc:0.91088+0.01196\ttest-auc:0.87568+0.02636\n",
      "[11]\ttrain-auc:0.91107+0.01175\ttest-auc:0.87579+0.02733\n",
      "[12]\ttrain-auc:0.91240+0.01100\ttest-auc:0.87664+0.02803\n",
      "[13]\ttrain-auc:0.91317+0.01063\ttest-auc:0.87616+0.02842\n",
      "[14]\ttrain-auc:0.91357+0.01029\ttest-auc:0.87605+0.02841\n",
      "[15]\ttrain-auc:0.91485+0.00946\ttest-auc:0.87633+0.02927\n",
      "[16]\ttrain-auc:0.91736+0.01229\ttest-auc:0.87556+0.03045\n",
      "[17]\ttrain-auc:0.91762+0.01178\ttest-auc:0.87547+0.03036\n",
      "[18]\ttrain-auc:0.92112+0.00968\ttest-auc:0.87348+0.02841\n",
      "[19]\ttrain-auc:0.92154+0.00966\ttest-auc:0.87345+0.02825\n",
      "[20]\ttrain-auc:0.92334+0.01131\ttest-auc:0.87293+0.02831\n",
      "[21]\ttrain-auc:0.92337+0.01145\ttest-auc:0.87256+0.02899\n",
      "[22]\ttrain-auc:0.92495+0.01192\ttest-auc:0.87269+0.02921\n",
      "[23]\ttrain-auc:0.92566+0.01168\ttest-auc:0.87340+0.02999\n",
      "[24]\ttrain-auc:0.92547+0.01160\ttest-auc:0.87408+0.03010\n",
      "[25]\ttrain-auc:0.92534+0.01174\ttest-auc:0.87358+0.03057\n",
      "[26]\ttrain-auc:0.92619+0.01128\ttest-auc:0.87382+0.03049\n",
      "[27]\ttrain-auc:0.92697+0.01084\ttest-auc:0.87357+0.03093\n",
      "[28]\ttrain-auc:0.92715+0.01115\ttest-auc:0.87358+0.03032\n",
      "[29]\ttrain-auc:0.92787+0.01098\ttest-auc:0.87347+0.02940\n",
      "[30]\ttrain-auc:0.92910+0.01067\ttest-auc:0.87295+0.02907\n",
      "[31]\ttrain-auc:0.92923+0.01061\ttest-auc:0.87233+0.02897\n",
      "[32]\ttrain-auc:0.92911+0.01050\ttest-auc:0.87234+0.02932\n",
      "[33]\ttrain-auc:0.92998+0.01037\ttest-auc:0.87175+0.02952\n",
      "[34]\ttrain-auc:0.93014+0.01049\ttest-auc:0.87175+0.02942\n",
      "[35]\ttrain-auc:0.93110+0.01076\ttest-auc:0.87121+0.02983\n",
      "[36]\ttrain-auc:0.93120+0.01049\ttest-auc:0.87141+0.02952\n",
      "[37]\ttrain-auc:0.93218+0.01045\ttest-auc:0.87184+0.02940\n",
      "[38]\ttrain-auc:0.93241+0.01065\ttest-auc:0.87308+0.02883\n",
      "[39]\ttrain-auc:0.93325+0.01002\ttest-auc:0.87173+0.02823\n",
      "[40]\ttrain-auc:0.93361+0.00995\ttest-auc:0.87175+0.02835\n",
      "[41]\ttrain-auc:0.93413+0.01046\ttest-auc:0.87259+0.02738\n",
      "[42]\ttrain-auc:0.93442+0.01034\ttest-auc:0.87242+0.02662\n",
      "[43]\ttrain-auc:0.93528+0.01006\ttest-auc:0.87283+0.02693\n",
      "[44]\ttrain-auc:0.93575+0.01017\ttest-auc:0.87341+0.02621\n",
      "[45]\ttrain-auc:0.93593+0.01053\ttest-auc:0.87322+0.02638\n",
      "[46]\ttrain-auc:0.93657+0.01036\ttest-auc:0.87416+0.02577\n",
      "[47]\ttrain-auc:0.93707+0.00991\ttest-auc:0.87443+0.02545\n",
      "[48]\ttrain-auc:0.93781+0.01009\ttest-auc:0.87419+0.02472\n",
      "[49]\ttrain-auc:0.93798+0.00984\ttest-auc:0.87398+0.02462\n",
      "[13:08:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.87802+0.01519\ttest-auc:0.84580+0.02552\n",
      "[1]\ttrain-auc:0.89247+0.00894\ttest-auc:0.86217+0.03445\n",
      "[2]\ttrain-auc:0.89625+0.00817\ttest-auc:0.86705+0.03919\n",
      "[3]\ttrain-auc:0.90002+0.00825\ttest-auc:0.86976+0.03895\n",
      "[4]\ttrain-auc:0.90321+0.01192\ttest-auc:0.87176+0.03318\n",
      "[5]\ttrain-auc:0.90559+0.01277\ttest-auc:0.87353+0.03114\n",
      "[6]\ttrain-auc:0.90567+0.01297\ttest-auc:0.87443+0.03095\n",
      "[7]\ttrain-auc:0.90629+0.01295\ttest-auc:0.87515+0.03041\n",
      "[8]\ttrain-auc:0.90618+0.01240\ttest-auc:0.87532+0.03018\n",
      "[9]\ttrain-auc:0.90610+0.01283\ttest-auc:0.87654+0.03078\n",
      "[10]\ttrain-auc:0.90580+0.01289\ttest-auc:0.87658+0.03000\n",
      "[11]\ttrain-auc:0.90589+0.01309\ttest-auc:0.87706+0.03035\n",
      "[12]\ttrain-auc:0.90674+0.01283\ttest-auc:0.87838+0.03001\n",
      "[13]\ttrain-auc:0.90652+0.01295\ttest-auc:0.87771+0.03056\n",
      "[14]\ttrain-auc:0.90712+0.01288\ttest-auc:0.87727+0.03064\n",
      "[15]\ttrain-auc:0.90744+0.01279\ttest-auc:0.87698+0.03089\n",
      "[16]\ttrain-auc:0.90781+0.01223\ttest-auc:0.87720+0.03098\n",
      "[17]\ttrain-auc:0.90763+0.01193\ttest-auc:0.87723+0.03051\n",
      "[18]\ttrain-auc:0.90774+0.01182\ttest-auc:0.87748+0.03046\n",
      "[19]\ttrain-auc:0.90770+0.01199\ttest-auc:0.87778+0.03041\n",
      "[20]\ttrain-auc:0.90838+0.01321\ttest-auc:0.87782+0.03061\n",
      "[21]\ttrain-auc:0.90835+0.01318\ttest-auc:0.87789+0.03077\n",
      "[22]\ttrain-auc:0.90833+0.01297\ttest-auc:0.87780+0.03068\n",
      "[23]\ttrain-auc:0.90842+0.01282\ttest-auc:0.87767+0.03046\n",
      "[24]\ttrain-auc:0.90800+0.01294\ttest-auc:0.87726+0.02995\n",
      "[25]\ttrain-auc:0.90784+0.01321\ttest-auc:0.87742+0.03016\n",
      "[26]\ttrain-auc:0.90856+0.01273\ttest-auc:0.87702+0.03033\n",
      "[27]\ttrain-auc:0.90857+0.01265\ttest-auc:0.87696+0.03034\n",
      "[28]\ttrain-auc:0.90829+0.01279\ttest-auc:0.87707+0.03031\n",
      "[29]\ttrain-auc:0.90832+0.01298\ttest-auc:0.87699+0.03064\n",
      "[30]\ttrain-auc:0.90821+0.01296\ttest-auc:0.87723+0.03068\n",
      "[31]\ttrain-auc:0.90822+0.01297\ttest-auc:0.87697+0.03034\n",
      "[32]\ttrain-auc:0.90814+0.01285\ttest-auc:0.87667+0.03024\n",
      "[33]\ttrain-auc:0.90845+0.01265\ttest-auc:0.87683+0.03046\n",
      "[34]\ttrain-auc:0.90856+0.01282\ttest-auc:0.87676+0.03049\n",
      "[35]\ttrain-auc:0.90840+0.01287\ttest-auc:0.87684+0.03042\n",
      "[36]\ttrain-auc:0.90848+0.01285\ttest-auc:0.87696+0.03019\n",
      "[37]\ttrain-auc:0.90833+0.01278\ttest-auc:0.87695+0.03004\n",
      "[38]\ttrain-auc:0.90864+0.01258\ttest-auc:0.87717+0.03007\n",
      "[39]\ttrain-auc:0.90877+0.01264\ttest-auc:0.87698+0.03021\n",
      "[40]\ttrain-auc:0.90941+0.01239\ttest-auc:0.87627+0.03015\n",
      "[41]\ttrain-auc:0.90942+0.01244\ttest-auc:0.87635+0.03011\n",
      "[42]\ttrain-auc:0.90955+0.01241\ttest-auc:0.87604+0.03021\n",
      "[43]\ttrain-auc:0.90955+0.01242\ttest-auc:0.87627+0.03010\n",
      "[44]\ttrain-auc:0.90952+0.01243\ttest-auc:0.87634+0.02999\n",
      "[45]\ttrain-auc:0.90950+0.01243\ttest-auc:0.87596+0.03019\n",
      "[46]\ttrain-auc:0.90943+0.01250\ttest-auc:0.87609+0.03030\n",
      "[47]\ttrain-auc:0.90954+0.01263\ttest-auc:0.87583+0.03016\n",
      "[48]\ttrain-auc:0.90958+0.01264\ttest-auc:0.87560+0.03047\n",
      "[49]\ttrain-auc:0.90947+0.01282\ttest-auc:0.87559+0.03049\n",
      "[13:08:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.87802+0.01519\ttest-auc:0.84580+0.02552\n",
      "[1]\ttrain-auc:0.89248+0.00893\ttest-auc:0.86217+0.03445\n",
      "[2]\ttrain-auc:0.89625+0.00817\ttest-auc:0.86705+0.03919\n",
      "[3]\ttrain-auc:0.90004+0.00824\ttest-auc:0.86994+0.03888\n",
      "[4]\ttrain-auc:0.90331+0.01183\ttest-auc:0.87191+0.03313\n",
      "[5]\ttrain-auc:0.90566+0.01276\ttest-auc:0.87396+0.03131\n",
      "[6]\ttrain-auc:0.90625+0.01271\ttest-auc:0.87437+0.03032\n",
      "[7]\ttrain-auc:0.90705+0.01262\ttest-auc:0.87510+0.02971\n",
      "[8]\ttrain-auc:0.90695+0.01221\ttest-auc:0.87484+0.02948\n",
      "[9]\ttrain-auc:0.90704+0.01224\ttest-auc:0.87544+0.02934\n",
      "[10]\ttrain-auc:0.90695+0.01228\ttest-auc:0.87568+0.02870\n",
      "[11]\ttrain-auc:0.90712+0.01258\ttest-auc:0.87640+0.02930\n",
      "[12]\ttrain-auc:0.90864+0.01156\ttest-auc:0.87739+0.02884\n",
      "[13]\ttrain-auc:0.90860+0.01138\ttest-auc:0.87668+0.02933\n",
      "[14]\ttrain-auc:0.90904+0.01133\ttest-auc:0.87606+0.02907\n",
      "[15]\ttrain-auc:0.90944+0.01127\ttest-auc:0.87597+0.02933\n",
      "[16]\ttrain-auc:0.90982+0.01095\ttest-auc:0.87605+0.02970\n",
      "[17]\ttrain-auc:0.90959+0.01068\ttest-auc:0.87616+0.02933\n",
      "[18]\ttrain-auc:0.91167+0.00912\ttest-auc:0.87422+0.02737\n",
      "[19]\ttrain-auc:0.91163+0.00934\ttest-auc:0.87470+0.02737\n",
      "[20]\ttrain-auc:0.91223+0.01087\ttest-auc:0.87429+0.02735\n",
      "[21]\ttrain-auc:0.91227+0.01085\ttest-auc:0.87439+0.02734\n",
      "[22]\ttrain-auc:0.91229+0.01096\ttest-auc:0.87414+0.02731\n",
      "[23]\ttrain-auc:0.91280+0.01068\ttest-auc:0.87406+0.02719\n",
      "[24]\ttrain-auc:0.91145+0.01160\ttest-auc:0.87549+0.02805\n",
      "[25]\ttrain-auc:0.91147+0.01183\ttest-auc:0.87553+0.02810\n",
      "[26]\ttrain-auc:0.91197+0.01152\ttest-auc:0.87516+0.02854\n",
      "[27]\ttrain-auc:0.91282+0.01119\ttest-auc:0.87550+0.02917\n",
      "[28]\ttrain-auc:0.91245+0.01135\ttest-auc:0.87578+0.02908\n",
      "[29]\ttrain-auc:0.91270+0.01138\ttest-auc:0.87567+0.02920\n",
      "[30]\ttrain-auc:0.91273+0.01170\ttest-auc:0.87519+0.03001\n",
      "[31]\ttrain-auc:0.91273+0.01165\ttest-auc:0.87484+0.02989\n",
      "[32]\ttrain-auc:0.91276+0.01140\ttest-auc:0.87487+0.02993\n",
      "[33]\ttrain-auc:0.91304+0.01126\ttest-auc:0.87464+0.02994\n",
      "[34]\ttrain-auc:0.91311+0.01156\ttest-auc:0.87479+0.02986\n",
      "[35]\ttrain-auc:0.91294+0.01143\ttest-auc:0.87487+0.02975\n",
      "[36]\ttrain-auc:0.91303+0.01142\ttest-auc:0.87466+0.02957\n",
      "[37]\ttrain-auc:0.91319+0.01133\ttest-auc:0.87477+0.02959\n",
      "[38]\ttrain-auc:0.91317+0.01140\ttest-auc:0.87497+0.02960\n",
      "[39]\ttrain-auc:0.91316+0.01134\ttest-auc:0.87508+0.02968\n",
      "[40]\ttrain-auc:0.91321+0.01145\ttest-auc:0.87521+0.02988\n",
      "[41]\ttrain-auc:0.91311+0.01136\ttest-auc:0.87527+0.02989\n",
      "[42]\ttrain-auc:0.91323+0.01141\ttest-auc:0.87496+0.03028\n",
      "[43]\ttrain-auc:0.91336+0.01136\ttest-auc:0.87493+0.03060\n",
      "[44]\ttrain-auc:0.91347+0.01123\ttest-auc:0.87501+0.03038\n",
      "[45]\ttrain-auc:0.91342+0.01130\ttest-auc:0.87499+0.03046\n",
      "[46]\ttrain-auc:0.91349+0.01128\ttest-auc:0.87499+0.03067\n",
      "[47]\ttrain-auc:0.91375+0.01142\ttest-auc:0.87470+0.03044\n",
      "[48]\ttrain-auc:0.91371+0.01145\ttest-auc:0.87467+0.03059\n",
      "[49]\ttrain-auc:0.91364+0.01152\ttest-auc:0.87452+0.03045\n",
      "[13:08:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:08:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.87802+0.01519\ttest-auc:0.84580+0.02552\n",
      "[1]\ttrain-auc:0.89252+0.00888\ttest-auc:0.86226+0.03453\n",
      "[2]\ttrain-auc:0.89676+0.00832\ttest-auc:0.86707+0.03898\n",
      "[3]\ttrain-auc:0.90372+0.00953\ttest-auc:0.87212+0.03459\n",
      "[4]\ttrain-auc:0.90655+0.01283\ttest-auc:0.87422+0.03037\n",
      "[5]\ttrain-auc:0.90765+0.01281\ttest-auc:0.87519+0.02966\n",
      "[6]\ttrain-auc:0.90871+0.01243\ttest-auc:0.87475+0.02857\n",
      "[7]\ttrain-auc:0.90849+0.01347\ttest-auc:0.87603+0.02661\n",
      "[8]\ttrain-auc:0.90897+0.01279\ttest-auc:0.87522+0.02600\n",
      "[9]\ttrain-auc:0.90976+0.01184\ttest-auc:0.87595+0.02678\n",
      "[10]\ttrain-auc:0.91088+0.01196\ttest-auc:0.87568+0.02636\n",
      "[11]\ttrain-auc:0.91107+0.01175\ttest-auc:0.87579+0.02733\n",
      "[12]\ttrain-auc:0.91240+0.01100\ttest-auc:0.87664+0.02803\n",
      "[13]\ttrain-auc:0.91317+0.01063\ttest-auc:0.87616+0.02842\n",
      "[14]\ttrain-auc:0.91357+0.01029\ttest-auc:0.87605+0.02841\n",
      "[15]\ttrain-auc:0.91485+0.00946\ttest-auc:0.87633+0.02927\n",
      "[16]\ttrain-auc:0.91736+0.01229\ttest-auc:0.87556+0.03045\n",
      "[17]\ttrain-auc:0.91762+0.01178\ttest-auc:0.87547+0.03036\n",
      "[18]\ttrain-auc:0.92112+0.00968\ttest-auc:0.87348+0.02841\n",
      "[19]\ttrain-auc:0.92154+0.00966\ttest-auc:0.87345+0.02825\n",
      "[20]\ttrain-auc:0.92334+0.01131\ttest-auc:0.87293+0.02831\n",
      "[21]\ttrain-auc:0.92337+0.01145\ttest-auc:0.87256+0.02899\n",
      "[22]\ttrain-auc:0.92495+0.01192\ttest-auc:0.87269+0.02921\n",
      "[23]\ttrain-auc:0.92566+0.01168\ttest-auc:0.87340+0.02999\n",
      "[24]\ttrain-auc:0.92547+0.01160\ttest-auc:0.87408+0.03010\n",
      "[25]\ttrain-auc:0.92534+0.01174\ttest-auc:0.87358+0.03057\n",
      "[26]\ttrain-auc:0.92619+0.01128\ttest-auc:0.87382+0.03049\n",
      "[27]\ttrain-auc:0.92697+0.01084\ttest-auc:0.87357+0.03093\n",
      "[28]\ttrain-auc:0.92715+0.01115\ttest-auc:0.87358+0.03032\n",
      "[29]\ttrain-auc:0.92787+0.01098\ttest-auc:0.87347+0.02940\n",
      "[30]\ttrain-auc:0.92910+0.01067\ttest-auc:0.87295+0.02907\n",
      "[31]\ttrain-auc:0.92923+0.01061\ttest-auc:0.87233+0.02897\n",
      "[32]\ttrain-auc:0.92911+0.01050\ttest-auc:0.87234+0.02932\n",
      "[33]\ttrain-auc:0.92998+0.01037\ttest-auc:0.87175+0.02952\n",
      "[34]\ttrain-auc:0.93014+0.01049\ttest-auc:0.87175+0.02942\n",
      "[35]\ttrain-auc:0.93110+0.01076\ttest-auc:0.87121+0.02983\n",
      "[36]\ttrain-auc:0.93120+0.01049\ttest-auc:0.87141+0.02952\n",
      "[37]\ttrain-auc:0.93218+0.01045\ttest-auc:0.87184+0.02940\n",
      "[38]\ttrain-auc:0.93241+0.01065\ttest-auc:0.87308+0.02883\n",
      "[39]\ttrain-auc:0.93325+0.01002\ttest-auc:0.87173+0.02823\n",
      "[40]\ttrain-auc:0.93361+0.00995\ttest-auc:0.87175+0.02835\n",
      "[41]\ttrain-auc:0.93413+0.01046\ttest-auc:0.87259+0.02738\n",
      "[42]\ttrain-auc:0.93442+0.01034\ttest-auc:0.87242+0.02662\n",
      "[43]\ttrain-auc:0.93528+0.01006\ttest-auc:0.87283+0.02693\n",
      "[44]\ttrain-auc:0.93575+0.01017\ttest-auc:0.87341+0.02621\n",
      "[45]\ttrain-auc:0.93593+0.01053\ttest-auc:0.87322+0.02638\n",
      "[46]\ttrain-auc:0.93657+0.01036\ttest-auc:0.87416+0.02577\n",
      "[47]\ttrain-auc:0.93707+0.00991\ttest-auc:0.87443+0.02545\n",
      "[48]\ttrain-auc:0.93781+0.01009\ttest-auc:0.87419+0.02472\n",
      "[49]\ttrain-auc:0.93798+0.00984\ttest-auc:0.87398+0.02462\n",
      "[13:09:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:09:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:09:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.87802+0.01519\ttest-auc:0.84580+0.02552\n",
      "[1]\ttrain-auc:0.89247+0.00894\ttest-auc:0.86217+0.03445\n",
      "[2]\ttrain-auc:0.89625+0.00817\ttest-auc:0.86705+0.03919\n",
      "[3]\ttrain-auc:0.90002+0.00825\ttest-auc:0.86976+0.03895\n",
      "[4]\ttrain-auc:0.90321+0.01192\ttest-auc:0.87176+0.03318\n",
      "[5]\ttrain-auc:0.90559+0.01277\ttest-auc:0.87353+0.03114\n",
      "[6]\ttrain-auc:0.90567+0.01297\ttest-auc:0.87443+0.03095\n",
      "[7]\ttrain-auc:0.90629+0.01295\ttest-auc:0.87515+0.03041\n",
      "[8]\ttrain-auc:0.90618+0.01240\ttest-auc:0.87532+0.03018\n",
      "[9]\ttrain-auc:0.90610+0.01283\ttest-auc:0.87654+0.03078\n",
      "[10]\ttrain-auc:0.90580+0.01289\ttest-auc:0.87658+0.03000\n",
      "[11]\ttrain-auc:0.90589+0.01309\ttest-auc:0.87706+0.03035\n",
      "[12]\ttrain-auc:0.90674+0.01283\ttest-auc:0.87838+0.03001\n",
      "[13]\ttrain-auc:0.90652+0.01295\ttest-auc:0.87771+0.03056\n",
      "[14]\ttrain-auc:0.90712+0.01288\ttest-auc:0.87727+0.03064\n",
      "[15]\ttrain-auc:0.90744+0.01279\ttest-auc:0.87698+0.03089\n",
      "[16]\ttrain-auc:0.90781+0.01223\ttest-auc:0.87720+0.03098\n",
      "[17]\ttrain-auc:0.90763+0.01193\ttest-auc:0.87723+0.03051\n",
      "[18]\ttrain-auc:0.90774+0.01182\ttest-auc:0.87748+0.03046\n",
      "[19]\ttrain-auc:0.90770+0.01199\ttest-auc:0.87778+0.03041\n",
      "[20]\ttrain-auc:0.90838+0.01321\ttest-auc:0.87782+0.03061\n",
      "[21]\ttrain-auc:0.90835+0.01318\ttest-auc:0.87789+0.03077\n",
      "[22]\ttrain-auc:0.90833+0.01297\ttest-auc:0.87780+0.03068\n",
      "[23]\ttrain-auc:0.90842+0.01282\ttest-auc:0.87767+0.03046\n",
      "[24]\ttrain-auc:0.90800+0.01294\ttest-auc:0.87726+0.02995\n",
      "[25]\ttrain-auc:0.90784+0.01321\ttest-auc:0.87742+0.03016\n",
      "[26]\ttrain-auc:0.90856+0.01273\ttest-auc:0.87702+0.03033\n",
      "[27]\ttrain-auc:0.90857+0.01265\ttest-auc:0.87696+0.03034\n",
      "[28]\ttrain-auc:0.90829+0.01279\ttest-auc:0.87707+0.03031\n",
      "[29]\ttrain-auc:0.90832+0.01298\ttest-auc:0.87699+0.03064\n",
      "[30]\ttrain-auc:0.90821+0.01296\ttest-auc:0.87723+0.03068\n",
      "[31]\ttrain-auc:0.90822+0.01297\ttest-auc:0.87697+0.03034\n",
      "[32]\ttrain-auc:0.90814+0.01285\ttest-auc:0.87667+0.03024\n",
      "[33]\ttrain-auc:0.90845+0.01265\ttest-auc:0.87683+0.03046\n",
      "[34]\ttrain-auc:0.90856+0.01282\ttest-auc:0.87676+0.03049\n",
      "[35]\ttrain-auc:0.90840+0.01287\ttest-auc:0.87684+0.03042\n",
      "[36]\ttrain-auc:0.90848+0.01285\ttest-auc:0.87696+0.03019\n",
      "[37]\ttrain-auc:0.90833+0.01278\ttest-auc:0.87695+0.03004\n",
      "[38]\ttrain-auc:0.90864+0.01258\ttest-auc:0.87717+0.03007\n",
      "[39]\ttrain-auc:0.90877+0.01264\ttest-auc:0.87698+0.03021\n",
      "[40]\ttrain-auc:0.90941+0.01239\ttest-auc:0.87627+0.03015\n",
      "[41]\ttrain-auc:0.90942+0.01244\ttest-auc:0.87635+0.03011\n",
      "[42]\ttrain-auc:0.90955+0.01241\ttest-auc:0.87604+0.03021\n",
      "[43]\ttrain-auc:0.90955+0.01242\ttest-auc:0.87627+0.03010\n",
      "[44]\ttrain-auc:0.90952+0.01243\ttest-auc:0.87634+0.02999\n",
      "[45]\ttrain-auc:0.90950+0.01243\ttest-auc:0.87596+0.03019\n",
      "[46]\ttrain-auc:0.90943+0.01250\ttest-auc:0.87609+0.03030\n",
      "[47]\ttrain-auc:0.90954+0.01263\ttest-auc:0.87583+0.03016\n",
      "[48]\ttrain-auc:0.90958+0.01264\ttest-auc:0.87560+0.03047\n",
      "[49]\ttrain-auc:0.90947+0.01282\ttest-auc:0.87559+0.03049\n",
      "[13:09:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:09:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:09:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.87802+0.01519\ttest-auc:0.84580+0.02552\n",
      "[1]\ttrain-auc:0.89248+0.00893\ttest-auc:0.86217+0.03445\n",
      "[2]\ttrain-auc:0.89625+0.00817\ttest-auc:0.86705+0.03919\n",
      "[3]\ttrain-auc:0.90004+0.00824\ttest-auc:0.86994+0.03888\n",
      "[4]\ttrain-auc:0.90331+0.01183\ttest-auc:0.87191+0.03313\n",
      "[5]\ttrain-auc:0.90566+0.01276\ttest-auc:0.87396+0.03131\n",
      "[6]\ttrain-auc:0.90625+0.01271\ttest-auc:0.87437+0.03032\n",
      "[7]\ttrain-auc:0.90705+0.01262\ttest-auc:0.87510+0.02971\n",
      "[8]\ttrain-auc:0.90695+0.01221\ttest-auc:0.87484+0.02948\n",
      "[9]\ttrain-auc:0.90704+0.01224\ttest-auc:0.87544+0.02934\n",
      "[10]\ttrain-auc:0.90695+0.01228\ttest-auc:0.87568+0.02870\n",
      "[11]\ttrain-auc:0.90712+0.01258\ttest-auc:0.87640+0.02930\n",
      "[12]\ttrain-auc:0.90864+0.01156\ttest-auc:0.87739+0.02884\n",
      "[13]\ttrain-auc:0.90860+0.01138\ttest-auc:0.87668+0.02933\n",
      "[14]\ttrain-auc:0.90904+0.01133\ttest-auc:0.87606+0.02907\n",
      "[15]\ttrain-auc:0.90944+0.01127\ttest-auc:0.87597+0.02933\n",
      "[16]\ttrain-auc:0.90982+0.01095\ttest-auc:0.87605+0.02970\n",
      "[17]\ttrain-auc:0.90959+0.01068\ttest-auc:0.87616+0.02933\n",
      "[18]\ttrain-auc:0.91167+0.00912\ttest-auc:0.87422+0.02737\n",
      "[19]\ttrain-auc:0.91163+0.00934\ttest-auc:0.87470+0.02737\n",
      "[20]\ttrain-auc:0.91223+0.01087\ttest-auc:0.87429+0.02735\n",
      "[21]\ttrain-auc:0.91227+0.01085\ttest-auc:0.87439+0.02734\n",
      "[22]\ttrain-auc:0.91229+0.01096\ttest-auc:0.87414+0.02731\n",
      "[23]\ttrain-auc:0.91280+0.01068\ttest-auc:0.87406+0.02719\n",
      "[24]\ttrain-auc:0.91145+0.01160\ttest-auc:0.87549+0.02805\n",
      "[25]\ttrain-auc:0.91147+0.01183\ttest-auc:0.87553+0.02810\n",
      "[26]\ttrain-auc:0.91197+0.01152\ttest-auc:0.87516+0.02854\n",
      "[27]\ttrain-auc:0.91282+0.01119\ttest-auc:0.87550+0.02917\n",
      "[28]\ttrain-auc:0.91245+0.01135\ttest-auc:0.87578+0.02908\n",
      "[29]\ttrain-auc:0.91270+0.01138\ttest-auc:0.87567+0.02920\n",
      "[30]\ttrain-auc:0.91273+0.01170\ttest-auc:0.87519+0.03001\n",
      "[31]\ttrain-auc:0.91273+0.01165\ttest-auc:0.87484+0.02989\n",
      "[32]\ttrain-auc:0.91276+0.01140\ttest-auc:0.87487+0.02993\n",
      "[33]\ttrain-auc:0.91304+0.01126\ttest-auc:0.87464+0.02994\n",
      "[34]\ttrain-auc:0.91311+0.01156\ttest-auc:0.87479+0.02986\n",
      "[35]\ttrain-auc:0.91294+0.01143\ttest-auc:0.87487+0.02975\n",
      "[36]\ttrain-auc:0.91303+0.01142\ttest-auc:0.87466+0.02957\n",
      "[37]\ttrain-auc:0.91319+0.01133\ttest-auc:0.87477+0.02959\n",
      "[38]\ttrain-auc:0.91317+0.01140\ttest-auc:0.87497+0.02960\n",
      "[39]\ttrain-auc:0.91316+0.01134\ttest-auc:0.87508+0.02968\n",
      "[40]\ttrain-auc:0.91321+0.01145\ttest-auc:0.87521+0.02988\n",
      "[41]\ttrain-auc:0.91311+0.01136\ttest-auc:0.87527+0.02989\n",
      "[42]\ttrain-auc:0.91323+0.01141\ttest-auc:0.87496+0.03028\n",
      "[43]\ttrain-auc:0.91336+0.01136\ttest-auc:0.87493+0.03060\n",
      "[44]\ttrain-auc:0.91347+0.01123\ttest-auc:0.87501+0.03038\n",
      "[45]\ttrain-auc:0.91342+0.01130\ttest-auc:0.87499+0.03046\n",
      "[46]\ttrain-auc:0.91349+0.01128\ttest-auc:0.87499+0.03067\n",
      "[47]\ttrain-auc:0.91375+0.01142\ttest-auc:0.87470+0.03044\n",
      "[48]\ttrain-auc:0.91371+0.01145\ttest-auc:0.87467+0.03059\n",
      "[49]\ttrain-auc:0.91364+0.01152\ttest-auc:0.87452+0.03045\n",
      "[13:09:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:09:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:09:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.87802+0.01519\ttest-auc:0.84580+0.02552\n",
      "[1]\ttrain-auc:0.89252+0.00888\ttest-auc:0.86226+0.03453\n",
      "[2]\ttrain-auc:0.89676+0.00832\ttest-auc:0.86707+0.03898\n",
      "[3]\ttrain-auc:0.90372+0.00953\ttest-auc:0.87212+0.03459\n",
      "[4]\ttrain-auc:0.90655+0.01283\ttest-auc:0.87422+0.03037\n",
      "[5]\ttrain-auc:0.90765+0.01281\ttest-auc:0.87519+0.02966\n",
      "[6]\ttrain-auc:0.90871+0.01243\ttest-auc:0.87475+0.02857\n",
      "[7]\ttrain-auc:0.90849+0.01347\ttest-auc:0.87603+0.02661\n",
      "[8]\ttrain-auc:0.90897+0.01279\ttest-auc:0.87522+0.02600\n",
      "[9]\ttrain-auc:0.90976+0.01184\ttest-auc:0.87595+0.02678\n",
      "[10]\ttrain-auc:0.91088+0.01196\ttest-auc:0.87568+0.02636\n",
      "[11]\ttrain-auc:0.91107+0.01175\ttest-auc:0.87579+0.02733\n",
      "[12]\ttrain-auc:0.91240+0.01100\ttest-auc:0.87664+0.02803\n",
      "[13]\ttrain-auc:0.91317+0.01063\ttest-auc:0.87616+0.02842\n",
      "[14]\ttrain-auc:0.91357+0.01029\ttest-auc:0.87605+0.02841\n",
      "[15]\ttrain-auc:0.91485+0.00946\ttest-auc:0.87633+0.02927\n",
      "[16]\ttrain-auc:0.91736+0.01229\ttest-auc:0.87556+0.03045\n",
      "[17]\ttrain-auc:0.91762+0.01178\ttest-auc:0.87547+0.03036\n",
      "[18]\ttrain-auc:0.92112+0.00968\ttest-auc:0.87348+0.02841\n",
      "[19]\ttrain-auc:0.92154+0.00966\ttest-auc:0.87345+0.02825\n",
      "[20]\ttrain-auc:0.92334+0.01131\ttest-auc:0.87293+0.02831\n",
      "[21]\ttrain-auc:0.92337+0.01145\ttest-auc:0.87256+0.02899\n",
      "[22]\ttrain-auc:0.92495+0.01192\ttest-auc:0.87269+0.02921\n",
      "[23]\ttrain-auc:0.92566+0.01168\ttest-auc:0.87340+0.02999\n",
      "[24]\ttrain-auc:0.92547+0.01160\ttest-auc:0.87408+0.03010\n",
      "[25]\ttrain-auc:0.92534+0.01174\ttest-auc:0.87358+0.03057\n",
      "[26]\ttrain-auc:0.92619+0.01128\ttest-auc:0.87382+0.03049\n",
      "[27]\ttrain-auc:0.92697+0.01084\ttest-auc:0.87357+0.03093\n",
      "[28]\ttrain-auc:0.92715+0.01115\ttest-auc:0.87358+0.03032\n",
      "[29]\ttrain-auc:0.92787+0.01098\ttest-auc:0.87347+0.02940\n",
      "[30]\ttrain-auc:0.92910+0.01067\ttest-auc:0.87295+0.02907\n",
      "[31]\ttrain-auc:0.92923+0.01061\ttest-auc:0.87233+0.02897\n",
      "[32]\ttrain-auc:0.92911+0.01050\ttest-auc:0.87234+0.02932\n",
      "[33]\ttrain-auc:0.92998+0.01037\ttest-auc:0.87175+0.02952\n",
      "[34]\ttrain-auc:0.93014+0.01049\ttest-auc:0.87175+0.02942\n",
      "[35]\ttrain-auc:0.93110+0.01076\ttest-auc:0.87121+0.02983\n",
      "[36]\ttrain-auc:0.93120+0.01049\ttest-auc:0.87141+0.02952\n",
      "[37]\ttrain-auc:0.93218+0.01045\ttest-auc:0.87184+0.02940\n",
      "[38]\ttrain-auc:0.93241+0.01065\ttest-auc:0.87308+0.02883\n",
      "[39]\ttrain-auc:0.93325+0.01002\ttest-auc:0.87173+0.02823\n",
      "[40]\ttrain-auc:0.93361+0.00995\ttest-auc:0.87175+0.02835\n",
      "[41]\ttrain-auc:0.93413+0.01046\ttest-auc:0.87259+0.02738\n",
      "[42]\ttrain-auc:0.93442+0.01034\ttest-auc:0.87242+0.02662\n",
      "[43]\ttrain-auc:0.93528+0.01006\ttest-auc:0.87283+0.02693\n",
      "[44]\ttrain-auc:0.93575+0.01017\ttest-auc:0.87341+0.02621\n",
      "[45]\ttrain-auc:0.93593+0.01053\ttest-auc:0.87322+0.02638\n",
      "[46]\ttrain-auc:0.93657+0.01036\ttest-auc:0.87416+0.02577\n",
      "[47]\ttrain-auc:0.93707+0.00991\ttest-auc:0.87443+0.02545\n",
      "[48]\ttrain-auc:0.93781+0.01009\ttest-auc:0.87419+0.02472\n",
      "[49]\ttrain-auc:0.93798+0.00984\ttest-auc:0.87398+0.02462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 13:09:05.238 | INFO     | trainer.model:train:90 - Best num_boost_round:3\n",
      "2022-01-05 13:09:05.240 | INFO     | trainer.model:train:93 - Best CV score: 0.87398\n",
      "2022-01-05 13:09:05.242 | INFO     | trainer.model:train:114 - Training XGBoost classifier with parameters {'objective': 'binary:logistic', 'eval_metric': 'auc', 'learning_rate': 0.001, 'n_estimators': 2000, 'max_depth': 4, 'min_child_weight': 0, 'gamma': 0, 'subsample': 0.7, 'colsample_bytree': 0.7, 'scale_pos_weight': 1, 'seed': 27, 'reg_alpha': 1e-05, 'early_stopping_rounds': 15}\n",
      "2022-01-05 13:09:05.246 | INFO     | trainer.model:train:121 - (881,)\n",
      "2022-01-05 13:09:05.247 | INFO     | trainer.model:train:122 - (881, 66)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:09:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_rounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n",
      "2022-01-05 13:09:11.970 | INFO     | trainer.model:train:124 - XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.7,\n",
      "              early_stopping_rounds=15, enable_categorical=False,\n",
      "              eval_metric='auc', gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.001, max_delta_step=0,\n",
      "              max_depth=4, min_child_weight=0, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=2000, n_jobs=4,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=27,\n",
      "              reg_alpha=1e-05, reg_lambda=1, scale_pos_weight=1, seed=27,\n",
      "              subsample=0.7, tree_method='exact', use_label_encoder=False, ...)\n",
      "2022-01-05 13:09:11.976 | INFO     | trainer.model:train:125 - Predict on the Test dataset.\n",
      "2022-01-05 13:09:12.049 | INFO     | trainer.model:train:130 - Evaluate model against Train dataset.\n",
      "2022-01-05 13:09:12.050 | INFO     | trainer.model:train:131 - Kaggle doesn't provide ground truth for test dataset.\n",
      "2022-01-05 13:09:12.055 | INFO     | trainer.model:train:134 - Model Precision 0.8622950819672132\n",
      "2022-01-05 13:09:12.059 | INFO     | trainer.model:train:135 - Model Recall: 0.7735294117647059\n",
      "2022-01-05 13:09:12.061 | INFO     | trainer.model:train:136 - Model Accuracy: 0.8649262202043133\n",
      "2022-01-05 13:09:12.065 | INFO     | trainer.model:train:137 - Model F1 score: 0.8155038759689923\n",
      "2022-01-05 13:09:12.068 | INFO     | trainer.model:train:138 - Model Log Loss: 4.66532119646588\n",
      "2022-01-05 13:09:12.071 | INFO     | trainer.model:train:139 - Model ROC Curve: (array([0.        , 0.07763401, 1.        ]), array([0.        , 0.77352941, 1.        ]), array([2, 1, 0]))\n",
      "2022-01-05 13:09:12.073 | INFO     | trainer.model:train:141 - Model tn, fp, fn, tp: [499  42  77 263]\n",
      "2022-01-05 13:09:12.076 | INFO     | trainer.model:train:144 - Model Area Under the Curve: 0.8479477003370665\n",
      "2022-01-05 13:09:12.077 | INFO     | trainer.model:train:146 - Test dataset Class Distribution: Counter({0: 541, 1: 340})\n",
      "2022-01-05 13:09:12.077 | INFO     | trainer.model:train:148 - XGBoost BinaryClassifier model training completed succesfully!\n",
      "2022-01-05 13:09:12.087 | INFO     | trainer.model:train:166 - Extract model feature importances.\n",
      "2022-01-05 13:09:12.091 | INFO     | trainer.model:train:170 - Store feature importances to dataframe.\n",
      "2022-01-05 13:09:12.093 | INFO     | trainer.model:train:174 - Store model evaluation statistics to dataframe.\n",
      "2022-01-05 13:09:12.097 | INFO     | trainer.model:train:178 - Store training job outputs to local disk before uploading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:09:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_rounds\", \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 13:09:12.316 | INFO     | trainer.model:train:195 - Upload model to gs://vertex_ai_demos1/experiments/titanic_classifier\n",
      "2022-01-05 13:09:12.317 | INFO     | trainer.model:train:198 - Checking if output Storage bucket exists.\n",
      "2022-01-05 13:09:12.395 | INFO     | trainer.model:train:204 - Start uploading training job results to output Storage bucket.\n",
      "2022-01-05 13:09:12.430 | INFO     | trainer.model:train:228 - Exception during task: file already exists\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/titanic_classifier/trainer/model.py\", line 208, in train\n",
      "    \"./\" + file_name, config.trainer_output_path + \"/\" +  file_name\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 584, in copy_v2\n",
      "    compat.path_to_bytes(src), compat.path_to_bytes(dst), overwrite)\n",
      "tensorflow.python.framework.errors_impl.AlreadyExistsError: file already exists\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/titanic_classifier/trainer/model.py\", line 208, in train\n",
      "    \"./\" + file_name, config.trainer_output_path + \"/\" +  file_name\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 584, in copy_v2\n",
      "    compat.path_to_bytes(src), compat.path_to_bytes(dst), overwrite)\n",
      "tensorflow.python.framework.errors_impl.AlreadyExistsError: file already exists\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_8873/70814321.py\", line 1, in <module>\n",
      "    main.train_model(options)\n",
      "  File \"/home/jupyter/titanic_classifier/trainer/main.py\", line 29, in train_model\n",
      "    model.train(config)\n",
      "  File \"/home/jupyter/titanic_classifier/trainer/model.py\", line 230, in train\n",
      "    sys.exit(255)\n",
      "SystemExit: 255\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/titanic_classifier/trainer/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    207\u001b[0m                     gfile.copy(\n\u001b[0;32m--> 208\u001b[0;31m                         \u001b[0;34m\"./\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_output_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mfile_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mcopy_v2\u001b[0;34m(src, dst, overwrite)\u001b[0m\n\u001b[1;32m    583\u001b[0m   _pywrap_file_io.CopyFile(\n\u001b[0;32m--> 584\u001b[0;31m       compat.path_to_bytes(src), compat.path_to_bytes(dst), overwrite)\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m: file already exists",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8873/70814321.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/titanic_classifier/trainer/main.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training job completed succesfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/titanic_classifier/trainer/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m# A non-zero exit code causes the training job to be marked as Failed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m: 255",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2069\u001b[0m                            'the full traceback.\\n']\n\u001b[1;32m   2070\u001b[0m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0;32m-> 2071\u001b[0;31m                                                                      value))\n\u001b[0m\u001b[1;32m   2072\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n\u001b[1;32m    632\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0mchained_exception_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m                 + out_list)\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1368\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             )\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1125\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "main.train_model(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38d059-4153-49bc-9417-d78e4cca5a73",
   "metadata": {},
   "source": [
    "# Congrats you've succesfully run your training package locally - now we'll do the training on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c2574-65b8-43d6-8aa1-09f4a7c0ccd8",
   "metadata": {},
   "source": [
    "1. First you need to package your training application\n",
    "\n",
    "    1.1 python setup.py sdist --formats=gztar\n",
    "    \n",
    "2. Then copy the training package to cloud storage\n",
    "\n",
    "    2.2 gsutil cp dist/trainer-0.1.tar.gz GCS_PATH_FOR_PYTHON_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cebf60-4499-4295-8cf9-4ec68e070040",
   "metadata": {},
   "source": [
    "To run the exact same script on Vertex AI you can run the train_on_cloud.sh shell script.\n",
    "\n",
    "Take a look at the script and the vertexconf.yml to understand the syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de795a-2cf2-4404-a71a-e1ee2cd33b20",
   "metadata": {},
   "source": [
    "# After you've run your code succesfully on Vertex AI then we'll try to do a training job run with Vertex AI federated dataset and prebuilt containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77403a80-72f1-479b-a83b-1d9f4421d6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
